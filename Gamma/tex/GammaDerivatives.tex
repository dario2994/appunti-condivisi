\section{Differenziazione della gamma}\label{dg}
Questa sezione è la prima a dover usare esplicitamente ragionamenti del tipo \emph{epsilon, delta} oppure comunque ragionamenti di ``basso livello''.
Per evitare di usare questi ragionamenti si dovrebbero applicare teoremi forti riguardo la possibilità
di scambiare tra loro gli operatori di integrale, di derivata e di limite.
Tuttavia questi teoremi prescindono dal programma di analisi I e perciò abbiamo deciso di trovare strade
che li evitino, mantenendo le dimostrazioni le più elementari possibili.

Dimostreremo che la Gamma è una funzione derivabile infinite volte ed espliciteremo le sue derivate.
Inoltre studieremo alcune proprieta della Digamma (derivata logaritmica della Gamma).

\begin{lemma}\label{dg:SenzaIntegrale}
	Per ogni $t\in \mathbb{R}^+,\ x> -1$ e $h\in\mathbb{R}$ tale che $x+h>-1$ vale:
	\begin{equation*}
		|t^{x+h}-t^x-h\log t\cdot t^x| \le h^2\log^2t\cdot\max\left(t^x,t^{x+h}\right)
	\end{equation*}
\end{lemma}
\begin{proof}
	Applicando il teorema di Lagrange due volte, ottengo che esistono $\xi_1,\xi_2$ con valore assoluto minore di $|h|$ tali che:
	\begin{equation}\label{dg:LagrangeApprox}
		t^{x+h}-t^x-h\log t\cdot t^x=h\log t\cdot t^{x+\xi_1}-h\log t\cdot t^x=h\log t\left(t^{x+\xi_1}-t^x\right)=h\xi_1\log^2 t\cdot t^{x+\xi_2}
	\end{equation}
	Però $h,\xi_1$ devono avere lo stesso segno per Lagrange, e quindi $h\xi_1\le h^2$ ed inoltre vale per la convessità dell'esponenziale che $t^{x+\xi_2}\le\max\left(t^x,t^{x+h}\right)$.
	Sostituendo questi risultati in \cref{dg:LagrangeApprox} e aggiungendo un valore assoluto ottengo:
	\begin{equation*}
		|t^{x+h}-t^x-h\log t\cdot t^x|\le h^2\log^2 t\cdot \max\left(t^x,t^{x+h}\right)
	\end{equation*}
	che è la tesi.
\end{proof}

\begin{theorem}\label{dg:GammaDerivata}
	La funzione Gamma è derivabile e la derivata rispetta:
	\begin{equation*}
		\Gamma'(x)=\int_0^{\infty} \log{t}\cdot t^{x-1}e^{-t}\de t
	\end{equation*}
\end{theorem}
\begin{proof}
	Definisco $\Gamma'$ esattamente come supposto nell'enunciato del teorema e dimostro che è la derivata.

	Sfruttando la monotonia dell'operatore integrale e \cref{dg:SenzaIntegrale} ho, per $x-1,h$ come nelle ipotesi di \cref{dg:SenzaIntegrale}:
	\begin{equation}\label{dg:ConIntegrale} \begin{split}
		|\Gamma(x+h)-\Gamma(x)-h\Gamma'(x)|&\le \int_0^{\infty}e^{-t}|t^{x-1+h}-t^{x-1}-h\log t\cdot t^{x-1}| \\
		&\le h^2\int_0^{\infty} e^{-t}\log^2 t\cdot \max\left(t^{x-1},t^{x-1+h}\right)
	\end{split}\end{equation}

	Ma ora è facile notare che, indifferentemente dal valore di $h$ (purchè sufficientemente piccolo), l'integrale $\int_0^{\infty} e^{-t}\log^2 t\cdot \max\left(t^{x-1},t^{x-1+h}\right)$ esiste finito e limitato, cioè è $\bigO(1)$ in $h$, e di conseguenza dalla \cref{dg:ConIntegrale} ottengo:
	\begin{equation*}
		|\Gamma(x+h)-\Gamma(x)-h\Gamma'(x)|=\bigO(h^2)
	\end{equation*}
	e questo dimostra che la derivata di $\Gamma$ è $\Gamma'$.
\end{proof}
\begin{corollary}\label{dg:GammaDerivataN}
	La derivata $n$-esima della Gamma rispetta:
	\begin{equation*}
		\Gamma^{(n)}(x)=\int_0^{\infty} \log^{n}{t}\cdot t^{x-1}e^{-t}\de t
	\end{equation*}
\end{corollary}
\begin{proof}
	Si dimostra agevolmente per induzione su $n$. In particolare il passo induttivo si svolge
	ripetendo pedissequamente la dimostrazione di \cref{dg:GammaDerivata}, solo sostituendo ovunque $t^{x-1}e^{-x}$
	con $\log^{n-1}{t}\cdot t^{x-1}e^{-x}$.
\end{proof}
\begin{lemma}\label{dg:SommaDer}
	Siano $f_i:\mathbb{R^+}\to\mathbb R$ (con $i=1,2,\dots$) funzioni tali che:
	\begin{itemize}
		\item $\forall\ i\in\mathbb{N} :\ f_i\in C^2$
		\item $\forall x\in \mathbb{R^+}$ la sommatoria $\sum_{i=1}^{\infty}f_i(x)$ converge.
		\item La sommatoria $\sum_{i=1}^{\infty} ||f_i''||$ converge.
	\end{itemize}
	
	Allora vale la seguente identità tra derivate:
	\begin{equation*}
		\frac{\de \sum_{i=1}^{\infty}f_i(x)}{\de x}=\sum_{i=1}^{\infty}f_i'(x)
	\end{equation*}
\end{lemma}
\begin{proof}
	Applicando due volte il teorema di Lagrange ho che fissati $x,h\in\mathbb{R^+}$ e $i\in\mathbb{N}$
	esistono $0<h''\le h'\le h$ tali che valgano le identità che uso per ottenere la disuguaglianza:
	\begin{equation}\label{dg:DisFond}
		\left\lvert\frac{f_i(x+h)-f_i(x)}h - f_i'(x)\right\rvert=\left\lvert f_i'(x+h') - f_i'(x)\right\rvert
		=\left\lvert h'\cdot f''(x+h'') \right\rvert\le h||f_i''||
	\end{equation}
	
	Sia ora $C=\sum_{i=1}^{\infty} ||f_i''||$ (che esiste per ipotesi).
	
	Risulta vera, sfruttando \cref{dg:DisFond}, che:
	\begin{equation}\label{dg:DerFond}
		hC\ge \sum_{i=1}^{\infty}\left\lvert\frac{f_i(x+h)-f_i(x)}h - f_i'(x)\right\rvert \ge
		\left\lvert\sum_{i=1}^{\infty}\frac{f_i(x+h)-f_i(x)}h - f_i'(x)\right\rvert
	\end{equation}
	Dove l'ultima disuguaglianza ha senso poichè la serie converge assolutamente, quindi converge.
	
	Vale però che una somma di serie equivale alla serie della somma e perciò
	(essendo furbi, portando le cose dalla parte giusta e applicando due volte tale risultato nell'ordine giusto)
	si dimostra che:
	\begin{equation}\label{dg:IdStupida}
		\sum_{i=1}^{\infty}\frac{f_i(x+h)-f_i(x)}h - f_i'(x)=
		\frac{\sum_{i=1}^{\infty} f_i(x+h)- \sum_{i=1}^{\infty}f_i(x)}h -\sum_{i=1}^{\infty}f_i'(x)
	\end{equation}

	Unendo \cref{dg:DerFond,dg:IdStupida} ottengo, per definizione di derivata, proprio la tesi.
\end{proof}

\begin{definition}\label{dg:Digamma}
	Sia $\psi:\mathbb{R^+}\to\mathbb{R}$ la funzione Digamma, cioè la derivata logaritmica della funzione Gamma:
	\begin{equation*}
		\psi(x)=\frac{\de \log\Gamma(x)}{\de x}=\frac{\Gamma'(x)}{\Gamma(x)}
	\end{equation*}
\end{definition}

\begin{lemma}\label{dg:DigammaCresc}
	La funzione Digamma è crescente.
\end{lemma}
\begin{proof}
	La \cref{GammaLogConvessa} mi assicura che la funzione $\log\Gamma$ è convessa, ma questa è per \cref{dg:GammaDerivata}
	anche derivabile. Ma una funzione derivabile e convessa ha derivata crescente, per la definizione stessa della Digamma
	questo implica la sua crescenza.
\end{proof}

\begin{theorem}\label{dg:DigammaFunzionale}
	La funzione Digamma rispetta la seguente equazione funzionale per ogni $x>0$:
	\begin{equation*}
		\psi(x+1)=\psi(x)+\frac 1x
	\end{equation*}
\end{theorem}
\begin{proof}
	Sfruttando \cref{dg:GammaDerivata} derivo entrambi i membri di \cref{FunzionaleGamma}:
	\begin{equation}\label{dg:FunzionaleDer}
		\Gamma'(x+1)=x\Gamma'(x)+\Gamma(x)
	\end{equation}
	
	Ora unisco \cref{FunzionaleGamma,dg:FunzionaleDer,dg:Digamma} ottenendo la tesi:
	\begin{equation*}
		\psi(x+1)=\frac{\Gamma'(x+1)}{\Gamma(x+1)}=\frac{x\Gamma'(x)+\Gamma(x)}{x\Gamma(x)}=
		\frac{\Gamma'(x)}{\Gamma(x)}+\frac 1x=\psi(x)+\frac 1x
	\end{equation*}
\end{proof}

\begin{theorem}\label{dg:DigammaId}
	La funzione Digamma rispetta la seguente identità per ogni $x>0$:
	\begin{equation*}
		\psi(x)=-\gamma-\frac 1x +\sum_{i=1}^{\infty} \frac x{i(i+x)}
	\end{equation*}
\end{theorem}
\begin{proof}
	Sfruttando \cref{WeierstrassFormula} e le proprietà di base del logaritmo si ha:
	\begin{equation}\label{dg:DigammaQuasi}
		\log\Gamma(x)=-\gamma x- \log{x} +\sum_{i=1}^{\infty}\left(\frac xi -\log\left(1+\frac xi\right)\right)
	\end{equation}
	
	Definisco ora $f_i:\mathbb{R^+}\to\mathbb{R}$ come $f_i(x)=\frac xi -\log\left(1+\frac xi\right)$.
	
	Verifico che le $f_i$ rispettino tutte le ipotesi di \cref{dg:SommaDer}:
	\begin{itemize}
		\item $f_i\in C^2$ e questo è ovvio vista la definizione delle $f_i$.
		\item $\sum_{i=1}^{\infty}f_i(x)$ converge sempre, ma anche questo è ovvio per la \cref{dg:DigammaQuasi}, visto che
		il membro di sinistra esiste finito e nel membro di destra compare questa sommatoria infinita (con un numero finito
		di altri addendi).
		\item Per verificare l'ultima ipotesi derivo due volte le $f_i$:
		\begin{equation}\label{dg:Der1Fi}
			f_i'(x)=\frac 1i -\frac{\frac 1i}{1+\frac xi}=\frac{x}{i(i+x)}
		\end{equation}
		\begin{equation*}
			f_i''(x)=\frac 1{i(i+x)} - \frac{x}{i(i+x)^2}=\frac 1{i(i+x)}\left(1-\frac x{i+x}\right)\le \frac 1{i(i+x)}\le \frac 1{i^2}
		\end{equation*}
		Ma allora (notando la facile positività di $f_i''$) ho che $||f_i''||\le \frac 1{i^2}$ e questo implica facilmente 
		l'ultima ipotesi di \cref{dg:SommaDer}.
	\end{itemize}
	
	Derivo entrambi i membri di \cref{dg:DigammaQuasi} e applico \cref{dg:SommaDer,dg:Der1Fi} (di cui ho appena verificato che siano rispettate
	le ipotesi):
	\begin{equation*}
		\psi(x)=\frac{\de \log\Gamma(x)}{\de x}=-\gamma-\frac 1x + \sum_{i=1}^{\infty}\frac{x}{i(i+x)}
	\end{equation*}
	che è la tesi.
\end{proof}

\begin{corollary}\label{dg:psi(1)}
	Risulta:
	\begin{equation*}
		\psi(1)=-\gamma
	\end{equation*}
\end{corollary}
\begin{proof}
	Ponendo $x=1$ in \cref{dg:DigammaId} ottengo:
	\begin{equation*}
		\psi(1)=-\gamma-1+\sum_{i=1}^{\infty}\frac 1{i(i+1)}
	\end{equation*}
	Ma la sommatoria infinita è, per antonomasia, telescopica e risulta valere $1$. Sostistuendo il valore della sommatoria
	ottengo proprio il risultato.
\end{proof}

\begin{theorem}\label{dg:psiApprox}
	Vale la seguente stima asintotica della funzione Digamma:
	\begin{equation*}
		\psi(x)=\log{x}+\bigO\left(\frac 1x\right)
	\end{equation*}
\end{theorem}
\begin{proof}
	Dato $n\in\mathbb{N}$ applicando ripetutamente la \cref{dg:DigammaFunzionale} e sostituendo \cref{dg:psi(1)}
	ho che vale:
	\begin{equation}\label{dg:PsiIntera}
		\psi(n)=\psi(1)+\sum_{i=1}^{n-1}\frac 1i = -\gamma+\sum_{i=1}^{n}\frac 1i + \bigO\left(\frac 1n\right)
	\end{equation}
	Ora però si può applicare il fatto noto riguardo la convergenza a $\gamma$ della differenza tra logaritmo e numeri armonici, 
	così si ottiene:
	\begin{equation}\label{dg:EuMaschConv}
		\sum_{i=1}^{n}\frac 1i - \log n = \gamma +\bigO\left(\frac 1n\right)
	\end{equation}
	
	Sostituisco ora \cref{dg:EuMaschConv} in \cref{dg:PsiIntera} ottenendo:
	\begin{equation}\label{dg:PsiIntera2}
		\psi(n)=\log n +\bigO\left(\frac 1n\right)
	\end{equation}

	Fissato $x>1$ reale, sfruttando \cref{dg:DigammaCresc,dg:DigammaFunzionale} ottengo la seguente catena di
	disuguaglianze (definendo per precisione $\lceil x\rceil$ come il più piccolo intero \emph{strettamente} maggiore di $x$):
	\begin{equation}\label{dg:PsiReale}
		\psi(\partint{x})\le \psi(x) \le \psi(\lceil x\rceil)=\psi(\partint{x})+\frac 1{\partint{x}} \Rightarrow
		\psi(x)=\psi(\partint{x})+\bigO\left(\frac 1x\right)
	\end{equation}

	Applico ora \cref{dg:PsiIntera2} nella \cref{dg:PsiReale} ottenendo la tesi:
	\begin{equation*}
		\psi(x)=\psi(\partint{x})+\bigO\left(\frac 1x\right)=
		\log{\partint{x} }+\bigO\left(\frac 1{\partint{x}}\right)+\bigO\left(\frac 1x\right)=
		\log{x}+\bigO\left(\frac 1x\right)
	\end{equation*}
\end{proof}


\begin{corollary}\label{dg:RapportoAsint}
	Per $c$ in un intervallo limitato di $\mathbb{R}$, vale la seguente formula asintotica per $x\to\infty$:
	\begin{equation}
		\frac{\Gamma(x+c)}{\Gamma(x)}\sim x^c
	\end{equation}
\end{corollary}
\begin{proof}
	Applicando due volte il teorema di Lagrange e sfruttando \cref{dg:psiApprox} ne ricavo che esistono $\xi_1,\xi_2$ tali che:
	\begin{equation}\label{dg:LogaritmoAsintotico}\begin{split}
		\log\Gamma(x+c)-\log\Gamma(x)&=c\cdot\psi(x+\xi_1)=c\left(\log(x+\xi_1)+\bigO\left(\frac 1{x+\xi_1}\right)\right)\\
		&=c\left(\log x+\frac{\xi_1}{x+\xi_2}+\bigO\left(\frac 1{x+\xi}\right)\right)=c\log x+\bigO\left(\frac1{x}\right)
	\end{split}\end{equation}
	dove nell'ultimo passaggio ho sfruttato il fatto che $c$ è limitato e quindi anche $\xi_1,\xi_2$.

	Ma ora facendo l'esponenziale a entrambi i membri di \cref{dg:LogaritmoAsintotico} ottengo proprio la tesi.
\end{proof}








