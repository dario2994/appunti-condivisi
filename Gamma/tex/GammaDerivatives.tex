\section{Differenziazione della gamma}\label{dg}
Questa sezione è la prima a dover usare esplicitamente ragionamenti del tipo \emph{epsilon, delta}.
Per evitare di usare questi ragionamenti si dovrebbero applicare teoremi forti riguardo la possibilità
di scambiare tra loro gli operatori di integrale, di derivata e di limite.
Tuttavia questi teoremi prescindono dal programma di analisi I e perciò abbiamo deciso di trovare strade
che li evitino, mantenendo le dimostrazioni le più elementari possibili.

Dimostreremo che la Gamma è una funzione derivabile infinite volte ed espliciteremo le sue derivate.
Inoltre studieremo alcune proprieta della Digamma (derivata logaritmica della Gamma).

\begin{definition}
	Sia $f_h:\mathbb{R}\to\mathbb{R}$, con $h\in\mathbb{R^+}$, la funzione definita come:
	\begin{equation*}
		f_h(t)=\frac{t^h-1}h
	\end{equation*}

\end{definition}

\begin{lemma}\label{dg:LagrangeApprox}
	Fissati $t,h\in\mathbb{R^+}$, esiste $0\le h'\le h$ tale che:
	\begin{equation*}
		f_h(t)=\log{t}\cdot t^{h'}
	\end{equation*}
\end{lemma}
\begin{proof}
	Sia $f:\mathbb{R}\to\mathbb{R}$ la funzione definita come $g(x)=t^x$.
	
	Applicando le regole standard di derivazione si ha $g'(x)=\log{t}\cdot t^x$.
	
	Applico il teorema di Lagrange con estremi $[0,h]$ alla funzione $g$ ottenendo la tesi:
	\begin{equation*}
		\exists 0\le h'\le h\ :\ f_h(t)=\frac{g(h)-g(0)}{h-0}=g'(h')=\log{t}\cdot t^{h'}
	\end{equation*}
\end{proof}



\begin{lemma}\label{dg:DisEstremale}
	Fissati $t,h\in\mathbb{R^+}$ con $h\le1$ risulta vera la disuguaglianza:
	\begin{equation*}
		\left\lvert f_h(t)-\log{t}\right\rvert\le \left\lvert\log{t}\right\rvert\max(1,t)
	\end{equation*}
\end{lemma}
\begin{proof}
	Applicando \cref{dg:LagrangeApprox} ottengo che vale la catena di identità (con $0<h'<h$):
	\begin{equation}\label{dg:FurbaId}
		\left\lvert f_h(t)-\log{t}\right\rvert=\left\lvert\log{t}\right\rvert \cdot \left\lvert t^{h'}-1\right\rvert
	\end{equation}
	
	Per $t\ge 1$ ho che vale (sfruttando $h'\le h\le 1$) $0\le t^{h'}-1\le t-1<t$.\\
	Per $0\le t<1$ ho che vale (sfruttando $h'\ge0$) $-1\le t^{h'}-1\le 0$.\\
	Unendo questi due risultati ottengo facilmente:
	\begin{equation}\label{dg:StupidaDis}
		\left\lvert t^{h'}-1\right\rvert \le \max(1,t)
	\end{equation}
	Applicando \cref{dg:StupidaDis} in \cref{dg:FurbaId} ottengo la tesi del lemma.
\end{proof}

\begin{lemma}\label{dg:UnifConv}
	Fissato un intervallo $[a,b]$ con $0<a<b$, per $h\to 0$ le funzioni $f_h$ convergono uniformemente alla funzione $\log$.
\end{lemma}
\begin{proof}
	Espandendo in serie di Taylor $e^{h\log{t}}$ e applicando il risultato per stimare le $f_h$ ottengo:
	\begin{equation*}
		f_h(t)=\frac{e^{h\log{t}}-1}h=\frac{1+h\log{t}+\bigO(h^2\log^2{t})-1}{h}=\log{t}+\bigO(h\log^2{t})=\log{t}+\bigO(h)
	\end{equation*}
	dove nell'ultimo passaggio ho sfruttato il fatto che siamo in $[a,b]$ e perciò $\log{t}$ risulta limitato.
	Questa equazione implica ovviamente l'uniforme convergenza enunciata nella tesi.

\end{proof}

\begin{theorem}\label{dg:GammaDerivata}
	La funzione Gamma è derivabile e la derivata rispetta:
	\begin{equation*}
		\Gamma'(x)=\int_0^{\infty} \log{t}\cdot t^{x-1}e^{-t}\de t
	\end{equation*}
\end{theorem}
\begin{proof}
	Come per la dimostrazione di \cref{GammaConverge}, l'integrale, che devo dimostrare
	essere la derivata della Gamma, esiste finito.
	
	Assunto che l'integrale esiste, la tesi del teorema è equivalente (per definizione di derivata) a dimostrare
	che per ogni $\varepsilon>0$ esiste $\delta$ (che scelgo minore di 1) tale che $\forall\ 0<h<\delta$ risulta:
	\begin{equation}\label{dg:EpsDeltaDerivata}
		\left\lvert 
		\frac{\Gamma(x+h)-\Gamma(x)}{h}-
		\int_0^{\infty} \log{t}\cdot t^{x-1}e^{-t}\de t
		\right\rvert \le \varepsilon
		\Longleftrightarrow
		\left\lvert
		\int_0^{\infty} \left(f_h(t)-\log{t}\right)t^{x-1}e^{-t}\de t
		\right\rvert \le \varepsilon
	\end{equation}
	
	Sia $0<a<1$ tale che:
	\begin{equation}\label{dg:ApproxIn0}
		\int_0^a \left\lvert \log(t)t^{x-1}e^{-t} \right\rvert \de t \le \frac{\varepsilon}3
	\end{equation}
	Tale $a$ esiste poichè l'integrale della \cref{dg:ApproxIn0} in 0 converge.
	
	Sia $b>\max(a,1)$ tale che:
	\begin{equation}\label{dg:ApproxInInf}
		\int_b^{\infty} \left\lvert (\log{t}\cdot t)t^{x-1}e^{-t}\right\rvert \de t \le \frac{\varepsilon}3
	\end{equation}
	Analogamente a prima, la $b$ esiste poichè l'integrale della \cref{dg:ApproxInInf} converge.
	
	Applicando i risultati \cref{dg:DisEstremale,dg:ApproxIn0,dg:ApproxInInf} ottengo le seguenti due identità:
	\begin{equation}\label{dg:IntIn0}
		\left\lvert \int_0^a \left(f_h(t)-\log{t}\right)t^{x-1}e^{-t}\de t\right\rvert
		\le
		\int_0^a \left\lvert f_h(t)-\log{t} \right\rvert t^{x-1}e^{-t}\de t
		\le
		\int_0^a \left\lvert \log{t} \right\rvert t^{x-1}e^{-t}\de t
		\le
		\frac{\varepsilon}3
	\end{equation}
	\begin{equation}\label{dg:IntInInf}
		\left\lvert \int_b^{\infty} \left(f_h(t)-\log{t}\right)t^{x-1}e^{-t}\de t\right\rvert
		\le
		\int_b^{\infty} \left\lvert f_h(t)-\log{t} \right\rvert t^{x-1}e^{-t}\de t
		\le
		\int_b^{\infty} \left\lvert \log{t}\cdot t\right\rvert t^{x-1}e^{-t}\de t
		\le
		\frac{\varepsilon}3
	\end{equation}
	
	Ora considero le funzioni $f_h$ ridotte all'intervallo $[a,b]$.
	L'uniforme convergenza mostrata in \cref{dg:UnifConv} e la limitatezza su $[a,b]$ di $t^{x-1}e^{-t}$ mi assicurano
	l'esistenza di $\delta>0$ tale che $\forall 0<h<\delta$ vale:
	\begin{equation}\label{dg:IntMiddle}
		\left\lvert \int_a^b \left(f_h(t)-\log{t}\right)t^{x-1}e^{-t}\de t\right\rvert \le \frac{\varepsilon}3
	\end{equation}
	Questo $\delta$ esiste poichè l'integrale converge a 0 per $h\to 0$.
	
	Unendo i risultati \cref{dg:IntIn0,dg:IntInInf,dg:IntMiddle}, con $h<\delta$ ottengo
	proprio la \cref{dg:EpsDeltaDerivata} e quindi la tesi:
	\begin{multline}
		\left\lvert
		\int_0^{\infty} \left(f_h(t)-\log{t}\right)t^{x-1}e^{-t}\de t
		\right\rvert \le \\
		\left\lvert \int_0^a \left(f_h(t)-\log{t}\right)t^{x-1}e^{-t}\de t\right\rvert +
		\left\lvert \int_a^b \left(f_h(t)-\log{t}\right)t^{x-1}e^{-t}\de t\right\rvert +
		\left\lvert \int_b^{\infty} \left(f_h(t)-\log{t}\right)t^{x-1}e^{-t}\de t\right\rvert \\
		\le \varepsilon
	\end{multline}
\end{proof}
\begin{corollary}\label{dg:GammaDerivataN}
	La derivata $n$-esima della Gamma rispetta:
	\begin{equation*}
		\Gamma^{(n)}(x)=\int_0^{\infty} \log^{n}{t}\cdot t^{x-1}e^{-t}\de t
	\end{equation*}
\end{corollary}
\begin{proof}
	Si dimostra agevolmente per induzione su $n$. In particolare il passo induttivo si svolge
	ripetendo pedissequamente la dimostrazione di \cref{dg:GammaDerivata}, solo sostituendo ovunque $t^{x-1}e^{-x}$
	con $\log^{n-1}{t}\cdot t^{x-1}e^{-x}$.
\end{proof}
\begin{lemma}\label{dg:SommaDer}
	Siano $f_i:\mathbb{R^+}\to\mathbb R$ (con $i=1,2,\dots$) funzioni tali che:
	\begin{itemize}
		\item $\forall\ i\in\mathbb{N} :\ f_i\in C^2$
		\item $\forall x\in \mathbb{R^+}$ la sommatoria $\sum_{i=1}^{\infty}f_i(x)$ converge.
		\item La sommatoria $\sum_{i=1}^{\infty} ||f_i''||$ converge.
	\end{itemize}
	
	Allora vale la seguente identità tra derivate:
	\begin{equation*}
		\frac{\de \sum_{i=1}^{\infty}f_i(x)}{\de x}=\sum_{i=1}^{\infty}f_i'(x)
	\end{equation*}
\end{lemma}
\begin{proof}
	Applicando due volte il teorema di Lagrange ho che fissati $x,h\in\mathbb{R^+}$ e $i\in\mathbb{N}$
	esistono $0<h''\le h'\le h$ tali che valgano le identità che uso per ottenere la disuguaglianza:
	\begin{equation}\label{dg:DisFond}
		\left\lvert\frac{f_i(x+h)-f_i(x)}h - f_i'(x)\right\rvert=\left\lvert f_i'(x+h') - f_i'(x)\right\rvert
		=\left\lvert h'\cdot f''(x+h'') \right\rvert\le h||f_i''||
	\end{equation}
	
	Sia ora $C=\sum_{i=1}^{\infty} ||f_i''||$ (che esiste per ipotesi).
	
	Risulta vera, sfruttando \cref{dg:DisFond}, che:
	\begin{equation}\label{dg:DerFond}
		hC\ge \sum_{i=1}^{\infty}\left\lvert\frac{f_i(x+h)-f_i(x)}h - f_i'(x)\right\rvert \ge
		\left\lvert\sum_{i=1}^{\infty}\frac{f_i(x+h)-f_i(x)}h - f_i'(x)\right\rvert
	\end{equation}
	Dove l'ultima disuguaglianza ha senso poichè la serie converge assolutamente, quindi converge.
	
	Vale però che una somma di serie equivale alla serie della somma e perciò
	(essendo furbi, portando le cose dalla parte giusta e applicando due volte tale risultato nell'ordine giusto)
	si dimostra che:
	\begin{equation}\label{dg:IdStupida}
		\sum_{i=1}^{\infty}\frac{f_i(x+h)-f_i(x)}h - f_i'(x)=
		\frac{\sum_{i=1}^{\infty} f_i(x+h)- \sum_{i=1}^{\infty}f_i(x)}h -\sum_{i=1}^{\infty}f_i'(x)
	\end{equation}

	Unendo \cref{dg:DerFond,dg:IdStupida} ottengo, per definizione di derivata, proprio la tesi.
\end{proof}

\begin{definition}\label{dg:Digamma}
	Sia $\psi:\mathbb{R^+}\to\mathbb{R}$ la funzione Digamma, cioè la derivata logaritmica della funzione Gamma:
	\begin{equation*}
		\psi(x)=\frac{\de \log\Gamma(x)}{\de x}=\frac{\Gamma'(x)}{\Gamma(x)}
	\end{equation*}
\end{definition}

\begin{lemma}\label{dg:DigammaCresc}
	La funzione Digamma è crescente.
\end{lemma}
\begin{proof}
	La \cref{GammaLogConvessa} mi assicura che la funzione $\log\Gamma$ è convessa, ma questa è per \cref{dg:GammaDerivata}
	anche derivabile. Ma una funzione derivabile e convessa ha derivata crescente, per la definizione stessa della Digamma
	questo implica la sua crescenza.
\end{proof}

\begin{theorem}\label{dg:DigammaFunzionale}
	La funzione Digamma rispetta la seguente equazione funzionale per ogni $x>0$:
	\begin{equation*}
		\psi(x+1)=\psi(x)+\frac 1x
	\end{equation*}
\end{theorem}
\begin{proof}
	Sfruttando \cref{dg:GammaDerivata} derivo entrambi i membri di \cref{FunzionaleGamma}:
	\begin{equation}\label{dg:FunzionaleDer}
		\Gamma'(x+1)=x\Gamma'(x)+\Gamma(x)
	\end{equation}
	
	Ora unisco \cref{FunzionaleGamma,dg:FunzionaleDer,dg:Digamma} ottenendo la tesi:
	\begin{equation*}
		\psi(x+1)=\frac{\Gamma'(x+1)}{\Gamma(x+1)}=\frac{x\Gamma'(x)+\Gamma(x)}{x\Gamma(x)}=
		\frac{\Gamma'(x)}{\Gamma(x)}+\frac 1x=\psi(x)+\frac 1x
	\end{equation*}
\end{proof}

\begin{theorem}\label{dg:DigammaId}
	La funzione Digamma rispetta la seguente identità per ogni $x>0$:
	\begin{equation*}
		\psi(x)=-\gamma-\frac 1x +\sum_{i=1}^{\infty} \frac x{i(i+x)}
	\end{equation*}
\end{theorem}
\begin{proof}
	Sfruttando \cref{WeierstrassFormula} e le proprietà di base del logaritmo si ha:
	\begin{equation}\label{dg:DigammaQuasi}
		\log\Gamma(x)=-\gamma x- \log{x} +\sum_{i=1}^{\infty}\left(\frac xi -\log\left(1+\frac xi\right)\right)
	\end{equation}
	
	Definisco ora $f_i:\mathbb{R^+}\to\mathbb{R}$ come $f_i(x)=\frac xi -\log\left(1+\frac xi\right)$.
	
	Verifico che le $f_i$ rispettino tutte le ipotesi di \cref{dg:SommaDer}:
	\begin{itemize}
		\item $f_i\in C^2$ e questo è ovvio vista la definizione delle $f_i$.
		\item $\sum_{i=1}^{\infty}f_i(x)$ converge sempre, ma anche questo è ovvio per la \cref{dg:DigammaQuasi}, visto che
		il membro di sinistra esiste finito e nel membro di destra compare questa sommatoria infinita (con un numero finito
		di altri addendi).
		\item Per verificare l'ultima ipotesi derivo due volte le $f_i$:
		\begin{equation}\label{dg:Der1Fi}
			f_i'(x)=\frac 1i -\frac{\frac 1i}{1+\frac xi}=\frac{x}{i(i+x)}
		\end{equation}
		\begin{equation*}
			f_i''(x)=\frac 1{i(i+x)} - \frac{x}{i(i+x)^2}=\frac 1{i(i+x)}\left(1-\frac x{i+x}\right)\le \frac 1{i(i+x)}\le \frac 1{i^2}
		\end{equation*}
		Ma allora (notando la facile positività di $f_i''$) ho che $||f_i''||\le \frac 1{i^2}$ e questo implica facilmente 
		l'ultima ipotesi di \cref{dg:SommaDer}.
	\end{itemize}
	
	Derivo entrambi i membri di \cref{dg:DigammaQuasi} e applico \cref{dg:SommaDer,dg:Der1Fi} (di cui ho appena verificato che siano rispettate
	le ipotesi):
	\begin{equation*}
		\psi(x)=\frac{\de \log\Gamma(x)}{\de x}=-\gamma-\frac 1x + \sum_{i=1}^{\infty}\frac{x}{i(i+x)}
	\end{equation*}
	che è la tesi.
\end{proof}

\begin{corollary}\label{dg:psi(1)}
	Risulta:
	\begin{equation*}
		\psi(1)=-\gamma
	\end{equation*}
\end{corollary}
\begin{proof}
	Ponendo $x=1$ in \cref{dg:DigammaId} ottengo:
	\begin{equation*}
		\psi(1)=-\gamma-1+\sum_{i=1}^{\infty}\frac 1{i(i+1)}
	\end{equation*}
	Ma la sommatoria infinita è, per antonomasia, telescopica e risulta valere $1$. Sostistuendo il valore della sommatoria
	ottengo proprio il risultato.
\end{proof}

\begin{theorem}\label{dg:psiApprox}
	Vale la seguente stima asintotica della funzione Digamma:
	\begin{equation*}
		\psi(x)=\log{x}+\bigO\left(\frac 1x\right)
	\end{equation*}
\end{theorem}
\begin{proof}
	Dato $n\in\mathbb{N}$ applicando ripetutamente la \cref{dg:DigammaFunzionale} e sostituendo \cref{dg:psi(1)}
	ho che vale:
	\begin{equation}\label{dg:PsiIntera}
		\psi(n)=\psi(1)+\sum_{i=1}^{n-1}\frac 1i = -\gamma+\sum_{i=1}^{n}\frac 1i + \bigO\left(\frac 1n\right)
	\end{equation}
	Ora però si può applicare il fatto noto riguardo la convergenza a $\gamma$ della differenza tra logaritmo e numeri armonici, 
	così si ottiene:
	\begin{equation}\label{dg:EuMaschConv}
		\sum_{i=1}^{n}\frac 1i - \log n = \gamma +\bigO\left(\frac 1n\right)
	\end{equation}
	
	Sostituisco ora \cref{dg:EuMaschConv} in \cref{dg:PsiIntera} ottenendo:
	\begin{equation}\label{dg:PsiIntera2}
		\psi(n)=\log n +\bigO\left(\frac 1n\right)
	\end{equation}

	Fissato $x>1$ reale, sfruttando \cref{dg:DigammaCresc,dg:DigammaFunzionale} ottengo la seguente catena di
	disuguaglianze (definendo per precisione $\lceil x\rceil$ come il più piccolo intero \emph{strettamente} maggiore di $x$):
	\begin{equation}\label{dg:PsiReale}
		\psi(\partint{x})\le \psi(x) \le \psi(\lceil x\rceil)=\psi(\partint{x})+\frac 1{\partint{x}} \Rightarrow
		\psi(x)=\psi(\partint{x})+\bigO\left(\frac 1x\right)
	\end{equation}

	Applico ora \cref{dg:PsiIntera2} nella \cref{dg:PsiReale} ottenendo la tesi:
	\begin{equation*}
		\psi(x)=\psi(\partint{x})+\bigO\left(\frac 1x\right)=
		\log{\partint{x} }+\bigO\left(\frac 1{\partint{x}}\right)+\bigO\left(\frac 1x\right)=
		\log{x}+\bigO\left(\frac 1x\right)
	\end{equation*}
\end{proof}


\begin{corollary}\label{dg:RapportoAsint}
	Fissato $c\in\mathbb{R}$, vale la seguente formula asintotica:
	\begin{equation}
		\frac{\Gamma(x+c)}{\Gamma(x)}\sim x^c
	\end{equation}
\end{corollary}
\begin{proof}
	Estraendo il logaritmo, la tesi equivale a dimostrare:
	\begin{equation}\label{dg:LogTesi}
		\lim_{x\to\infty} \log\Gamma(x+c)-\log\Gamma(x)-c\log x =0
	\end{equation}
	
	Applicando il teorema di Lagrange e \cref{dg:psiApprox} si ha che, per ogni $x>0$, esiste $0<c'<c$ (assumendo senza
	perdità di generalità $c>0$) tale che valgano le seguenti uguaglianze:
	\begin{equation}\label{dg:psiLagrange}
		\log\Gamma(x+c)-\log\Gamma(x)=c\cdot\psi(x+c')=c\log{x}+c\cdot\bigO\left(\frac{1+c'}{x}\right)=
		c\cdot \log{x}+\bigO\left(\frac 1x\right)
	\end{equation}
	dove nell'ultimo passaggio ho usato il fatto che $c$ è costante, e nell'identità centrale ho implicitamente 
	applicato Lagrange alla funzione logaritmo.
	
	Applicando la \cref{dg:psiLagrange} si dimostra facilmente \cref{dg:LogTesi} e quindi la tesi.
\end{proof}








